{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Exploratory Data Analysis (EDA)\n",
    "\n",
    "This notebook contains an initial exploratory analysis of the dataset.  \n",
    "\n",
    "The goal is to understand the structure, quality, and basic statistical properties of the data to guide further processing and modeling.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Load and preview the dataset\n",
    "- Understand the structure and types of data\n",
    "- Identify missing values and potential data quality issues\n",
    "- Generate basic descriptive statistics\n",
    "- Visualize key distributions and relationships\n",
    "\n",
    "> üß≠ This notebook serves as the starting point for any data-driven workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "\n",
    "We import the core libraries needed for data analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization styles\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.2)  # Seaborn style\n",
    "plt.style.use('ggplot')  # Matplotlib style (can be changed)\n",
    "\n",
    "# Optional: adjust figure aesthetics\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "\n",
    "# Uncomment to list all available matplotlib styles\n",
    "# print(plt.style.available)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "\n",
    "We load the dataset from a CSV file located in the `data/raw/` folder.  \n",
    "Multiple encoding formats are handled to prevent read errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the relative path to the data file\n",
    "data_path = '../data/raw/dataset.csv'  # Replace with your actual filename\n",
    "\n",
    "# Load the dataset with encoding fallback\n",
    "try:\n",
    "    df = pd.read_csv(data_path, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    try:\n",
    "        df = pd.read_csv(data_path, encoding='latin1')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading CSV file: {e}\")\n",
    "        df = None  # Use None to indicate failure\n",
    "\n",
    "# Check if the dataset was loaded successfully\n",
    "if df is not None:\n",
    "    print(f\"‚úÖ Dataset loaded successfully. Shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to load dataset. Please check the file path and encoding.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Look at the Dataset\n",
    "\n",
    "We display the first and last few rows to get an initial sense of the data structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the dataset\n",
    "if df is not None:\n",
    "    print(\"üîç First 5 rows of the dataset:\")\n",
    "    display(df.head())\n",
    "\n",
    "    print(\"\\nüîé Last 5 rows of the dataset:\")\n",
    "    display(df.tail())\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot preview data because the dataset failed to load.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "We inspect general metadata such as column names, data types, and non-null values to understand the dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information\n",
    "if df is not None:\n",
    "    print(\"üßæ Dataset Info:\")\n",
    "    df.info()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dataset information is unavailable because it failed to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique Values by Column\n",
    "\n",
    "Next, we explore the unique values present in each column of the dataset. This helps us understand the types of data, categories, and overall diversity, which is essential for guiding further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display unique values for each column in a visually friendly format\n",
    "def show_unique_values(dataframe, exclude_columns=None):\n",
    "    if exclude_columns is None:\n",
    "        exclude_columns = []\n",
    "\n",
    "    unique_values = {}\n",
    "\n",
    "    for col in dataframe.columns:\n",
    "        if col not in exclude_columns:\n",
    "            values = dataframe[col].dropna().unique()\n",
    "            if len(values) > 0:\n",
    "                unique_values[col] = values\n",
    "\n",
    "    print(f\"\\nDisplaying unique values for {len(unique_values)} out of {len(dataframe.columns)} columns.\\n\")\n",
    "\n",
    "    for col, values in unique_values.items():\n",
    "        print(f\"\\n{'=' * 80}\\n{col} ({len(values)} unique values)\\n{'=' * 80}\")\n",
    "\n",
    "        max_display = 200\n",
    "        if len(values) > max_display:\n",
    "            print(f\"Showing the first {max_display} values (out of {len(values)} total):\\n\")\n",
    "            values_to_show = values[:max_display]\n",
    "        else:\n",
    "            values_to_show = values\n",
    "\n",
    "        for i, val in enumerate(values_to_show):\n",
    "            print(f\"  {i+1}. {val}\")\n",
    "\n",
    "# Apply the function to the dataset\n",
    "if df is not None:\n",
    "    show_unique_values(df, exclude_columns=['FECHA_CORTE', 'PER_OCU', 'PER_DECLA', 'PER_UBIC', 'PER_SA', 'EVENTOS'])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot display unique values because the dataset failed to load.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n",
    "\n",
    "We calculate descriptive statistics to understand the distribution, central tendency, and variability of the dataset's values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate descriptive statistics\n",
    "if df is not None:\n",
    "    print(\"Descriptive Statistics:\")\n",
    "    display(df.describe(include='all'))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot compute descriptive statistics because the dataset failed to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "We analyze the presence of missing values in the dataset. Understanding which columns contain null entries and their proportions helps identify data quality issues and guides preprocessing decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "if df is not None:\n",
    "    # Calculate the number of missing values per column\n",
    "    null_counts = df.isnull().sum()\n",
    "    \n",
    "    # Calculate the percentage of missing values\n",
    "    null_percentage = (null_counts / len(df)) * 100\n",
    "    \n",
    "    # Create a DataFrame with null information\n",
    "    null_info = pd.DataFrame({\n",
    "        'Missing Values': null_counts,\n",
    "        'Percentage (%)': null_percentage\n",
    "    })\n",
    "    \n",
    "    print(\"Missing Values Analysis:\")\n",
    "    display(null_info)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot analyze missing values because the dataset failed to load.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration and Visualization\n",
    "\n",
    "In this section, we analyze unique values, descriptive statistics, missing data, and the distribution of a selected categorical column to better understand the dataset structure and guide further data processing or modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categorical_distribution(dataframe, column_name, top_n=10, title=None):\n",
    "    \"\"\"\n",
    "    Analyze and visualize the distribution of a categorical column.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe: pandas DataFrame\n",
    "    - column_name: name of the categorical column to analyze\n",
    "    - top_n: number of top categories to visualize\n",
    "    - title: custom title for the chart\n",
    "    \"\"\"\n",
    "    if dataframe is None or column_name not in dataframe.columns:\n",
    "        print(f\"‚ö†Ô∏è Column '{column_name}' not found or dataset is not loaded.\")\n",
    "        return\n",
    "\n",
    "    # Frequency and percentage\n",
    "    count = dataframe[column_name].value_counts()\n",
    "    percent = (count / len(dataframe)) * 100\n",
    "\n",
    "    # Combine into a DataFrame\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Frequency': count,\n",
    "        'Percentage (%)': percent\n",
    "    })\n",
    "\n",
    "    print(f\"Distribution of '{column_name}':\")\n",
    "    display(summary_df)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(x=count.values[:top_n], y=count.index[:top_n])\n",
    "    plt.title(title or f\"Top {top_n} Most Frequent Categories in '{column_name}'\")\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel(column_name)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "if df is not None:\n",
    "    analyze_categorical_distribution(df, column_name='HECHO', top_n=10)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dataset not loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection in Count Variables\n",
    "\n",
    "This step visualizes count variables using boxplots to identify potential outliers.\n",
    "\n",
    "Boxplots summarize the distribution, showing medians, quartiles, and extreme values that may indicate anomalies.\n",
    "\n",
    "This helps inform further cleaning before analysis or modeling.\n",
    "\n",
    "Column names are examples and should be adapted to your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define count variables\n",
    "count_variables = ['Count_Occupied', 'Count_Declared', 'Count_Located', 'Count_Reported', 'EventCount']\n",
    "\n",
    "# Step 2: Visualize boxplot to detect outliers in count variables\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df[count_variables])\n",
    "plt.title(\"Boxplot of count variables to detect outliers\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Summary\n",
    "\n",
    "This table quantifies outliers detected in each count variable using the Interquartile Range (IQR) method. It shows the number and percentage of data points outside typical bounds, providing insight into the potential impact of outliers on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsualizaci√≥n del impacto de los outliers\n",
    "resumen_outliers = []\n",
    "\n",
    "for col in variables_conteo:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    total = len(df)\n",
    "    num_outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0]\n",
    "\n",
    "    resumen_outliers.append({\n",
    "        'Variable': col,\n",
    "        'Q1': round(Q1, 2),\n",
    "        'Q3': round(Q3, 2),\n",
    "        'IQR': round(IQR, 2),\n",
    "        'L√≠mite inferior': round(lower_bound, 2),\n",
    "        'L√≠mite superior': round(upper_bound, 2),\n",
    "        'Outliers detectados': num_outliers,\n",
    "        'Porcentaje Outliers': round((num_outliers / total) * 100, 2)\n",
    "    })\n",
    "\n",
    "resumen_df = pd.DataFrame(resumen_outliers)\n",
    "display(resumen_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumen_df.to_csv(\n",
    "    path_or_buf='../data/processed/resumen_outliers.csv',\n",
    "    sep=',',\n",
    "    na_rep='',\n",
    "    header=True,\n",
    "    index=False,\n",
    "    encoding='utf-8',\n",
    "    quoting=csv.QUOTE_MINIMAL,\n",
    "    lineterminator=os.linesep,\n",
    "    quotechar='\"',\n",
    "    decimal='.',\n",
    "    errors='strict'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
